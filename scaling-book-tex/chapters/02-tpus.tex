\chapter{How to Think About TPUs}

\section*{What Is a TPU?}
\addcontentsline{toc}{section}{What Is a TPU?}

\textbf{A TPU is basically a compute core that specializes in matrix multiplication (called a TensorCore) attached to a stack of fast memory (called high-bandwidth memory or HBM).} Here's a diagram:

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/tpu-chip.png}
\caption{The basic components of a TPU chip. The TensorCore is the gray left-hand box, containing the matrix-multiply unit (MXU), vector unit (VPU), and vector memory (VMEM).}
\end{figure}

You can think of the TensorCore as basically just being a really good matrix multiplication machine, but it has a few other functions worth noting. The TensorCore has three key units:

\begin{itemize}
\item The \textbf{MXU} (Matrix Multiply Unit) is the core of the TensorCore. For most TPU generations, it performs one \texttt{bfloat16[8,128] @ bf16[128,128] -> f32[8,128]} matrix multiply\footnote{TPU v6e (Trillium) has a 256x256 MXU, while all previous generations use 128x128} every 8 cycles using a systolic array (see Appendix B for details).
  \begin{itemize}
  \item This is about \texttt{5e13} bf16 FLOPs/s per MXU at 1.5GHz on TPU v5e. Most TensorCores have 2 or 4 MXUs, so e.g. the total bf16 FLOPs/s for TPU v5e is \texttt{2e14}.
  \item TPUs also support lower precision matmuls with higher throughput (e.g. each TPU v5e chip can do \texttt{4e14} int8 OPs/s).
  \end{itemize}

\item The \textbf{VPU} (Vector Processing Unit) performs general mathematical operations like ReLU activations or pointwise addition or multiplication between vectors. Reductions (sums) are also performed here. Appendix A provides more details.

\item \textbf{VMEM} (Vector Memory) is an on-chip scratchpad located in the TensorCore, close to the compute units. It is much smaller than HBM (for example, 128 MiB on TPU v5e) but has a much higher bandwidth to the MXU. VMEM operates somewhat like an L1/L2 cache on CPUs but is much larger and programmer-controlled. Data in HBM needs to be copied into VMEM before the TensorCore can do any computation with it.
\end{itemize}

\textbf{TPUs are very, very fast at matrix multiplication}. It's mainly what they do and they do it well. TPU v5p, one of the most powerful TPUs to date, can do \texttt{2.5e14} bf16 FLOPs / second / core or \texttt{5e14} bf16 FLOPs / sec / chip. A single pod of 8960 chips can do 4 exaflops / second. That's \emph{a lot}. That's one of the most powerful supercomputers in the world. And Google has a lot of them.\footnote{TPUs, and their systolic arrays in particular, are such powerful hardware accelerators because matrix multiplication is one of the few algorithms that uses $O(n^3)$ compute for $O(n^2)$ bytes. That makes it very easy for an ordinary ALU to be bottlenecked by compute and not by memory bandwidth.}

The diagram above also includes a few other components like SMEM and the scalar unit, which are used for control flow handling and are discussed briefly in Appendix A, but aren't crucial to understand. On the other hand, HBM is important and fairly simple:

\begin{itemize}
\item \textbf{HBM} (High Bandwidth Memory) is a big chunk of fast memory that stores tensors for use by the TensorCore. HBM usually has capacity on the order of tens of gigabytes (for example, TPU v5e has 16GiB of HBM).

  \begin{itemize}
  \item When needed for a computation, tensors are streamed out of HBM through VMEM (see below) into the MXU and the result is written from VMEM back to HBM.

  \item The bandwidth between HBM and the TensorCore (through VMEM) is known as ``HBM bandwidth'' (usually around 1-2TB/sec) and limits how fast computation can be done in memory-bound workloads.
  \end{itemize}
\end{itemize}

\textbf{Generally, all TPU operations are pipelined and overlapped.} To perform a matmul $X \cdot A \to Y$, a TPU would first need to copy chunks of matrices $A$ and $X$ from HBM into VMEM, then load them into the MXU which multiplies chunks of 8x128 (for $X$) and 128x128 (for $A$), then copy the result chunk by chunk back to HBM. To do this efficiently, the matmul is pipelined so the copies to/from VMEM are overlapped with the MXU work. This allows the MXU to continue working instead of waiting on memory transfers, keeping matmuls compute-bound, not memory-bound.

A matmul would look nearly identical except it would load into the MXU instead of the VPU/Vector unit, and the loads and stores would occur in a different order, since the same weight chunk is used for multiple chunks of activations. You can see chunks of data streaming into VMEM, then into the VREGs (vector registers), then into the Vector Unit, then back into VMEM and HBM. As we're about to see, if the load from HBM to VMEM is slower than the FLOPs in the Vector Unit (or MXU), we become ``bandwidth bound'' since we're starving the VPU or MXU of work.

\begin{takeawaybox}
TPUs are very simple. They load weights from HBM into VMEM, then from VMEM into a systolic array which can perform around 200 trillion multiply-adds per second. The HBM $\leftrightarrow$ VMEM and VMEM $\leftrightarrow$ systolic array bandwidths set fundamental limits on what computations TPUs can do efficiently.
\end{takeawaybox}

\textbf{VMEM and arithmetic intensity:} VMEM is much smaller than HBM but it has a much higher bandwidth to the MXU. As we saw in Section 1, this means if an algorithm can fit all its inputs/outputs in VMEM, it's much less likely to hit communication bottlenecks. This is particularly helpful when a computation has poor arithmetic intensity: VMEM bandwidth is around 22x higher than HBM bandwidth which means an MXU operation reading from/writing to VMEM requires an arithmetic intensity of only 10-20 to achieve peak FLOPs utilization. That means if we can fit our weights into VMEM instead of HBM, our matrix multiplications can be FLOPs bound at much smaller batch sizes. And it means algorithms that fundamentally have a lower arithmetic intensity can still be efficient. VMEM is just so small this is often a challenge.\footnote{We sometimes talk about VMEM prefetching, which refers to loading weights ahead of time in VMEM so we can mask the cost of loading for our matmuls. For instance, in a normal Transformer we can sometimes load our big feed-forward weights into VMEM during attention, which can hide the cost of the weight load if we're memory bandwidth bound. This requires our weights to be small enough or sharded enough to fit a single layer into VMEM with space to spare.}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/tpu-bandwidth.png}
\end{figure}

\textbf{A TPU chip typically (but not always) consists of two TPU cores which share memory and can be thought of as one large accelerator} with twice the FLOPs (known as a ``megacore'' configuration). This has been true since TPU v4. Older TPU chips have separate memory and are regarded as two separate accelerators (TPU v3 and older). Inference-optimized chips like the TPU v5e only have one TPU core per chip.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/cores.png}
\end{figure}

\textbf{Chips} are arranged in \textbf{sets of 4 on a `tray'} connected to a \textbf{CPU host via PCIe network.} This is the format most readers will be familiar with, 4 chips (8 cores, though usually treated as 4 logical megacores) exposed through Colab or a single TPU-VM. For inference chips like the TPU v5e, we have 2 trays per host, instead of 1, but also only 1 core per chip, giving us 8 chips = 8 cores.\footnote{On Cloud TPU VMs, each tray is exposed as part of a separate VM, so there are once again 4 cores visible.}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/pcie.png}
\end{figure}

\textbf{PCIe bandwidth is limited:} Like the HBM $\leftrightarrow$ VMEM link, the CPU $\leftrightarrow$ HBM PCIe connection has a specific bandwidth that limits how quickly you can load from host memory to HBM or vice-versa. PCIe bandwidth for TPU v4 is 16GB / second each way, for example, so close to 100x slower than HBM. We \emph{can} load/offload data into the host (CPU) RAM, but not very quickly.

\section*{TPU Networking}
\addcontentsline{toc}{section}{TPU Networking}

\textbf{Chips are connected to each other through the ICI network in a Pod}. In older generations (TPU v2 and TPU v3), inference chips (e.g., TPU v5e), and Trillium (TPU v6e), ICI (``inter-chip interconnects'') connects the 4 nearest neighbors (with edge links to form a 2D torus). TPU v4 and TPU v5p are connected to the nearest 6 neighbors (forming a 3D torus). Note these connections do \textbf{not} go through their hosts, they are direct links between chips.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/ici-wraparound.png}
\end{figure}

The toroidal structure reduces the maximum distance between any two nodes from $N$ to $N / 2$, making communication much faster. TPUs also have a ``twisted torus'' configuration that wraps the torus in a Mobius-strip like topology to further reduce the average distance between nodes.

\textbf{TPU pods (connected by ICI) can get really big:} the maximum pod size (called a \textbf{superpod}) is \texttt{16x16x16} for TPU v4 and \texttt{16x20x28} for TPU v5p. These large pods are composed of reconfigurable cubes of \texttt{4x4x4} chips connected by optical wraparound links\footnote{The optical switch is simply a reconfigurable connection with the same ICI bandwidth. It just lets us connect cubes while retaining a wraparound link.} that we can reconfigure to connect very large topologies.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/tpu-rack.png}
\end{figure}

Smaller topologies (e.g. \texttt{2x2x1}, \texttt{2x2x2}) can also be requested, albeit with no wraparounds. This is an important caveat, since it typically doubles the time of most communication. Any multiple of a full cube (e.g. \texttt{4x4x4} or \texttt{4x4x8}) will have wraparounds provided by the optical switches.\footnote{Note that a \texttt{2x2x4} won't have any wraparounds since they are provided by the optical switches which are only available on a full cube. A TPU v5e 8x16 \emph{will} have a wraparound on the longer axis, however, since it doesn't use reconfigurable optical networking.}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/subslices.png}
\end{figure}

TPU v5e and Trillium pods consist of a single \texttt{16x16} 2D torus with wraparounds along any axis of size 16 (meaning an \texttt{8x16} has a wraparound on the long axis). TPUs v5e and v6e (Trillium) cannot expand beyond a 16x16 torus but pods can still communicate with each other over standard data-center networking (DCN), which connects TPU hosts to each other. Again, smaller topologies can be requested without wraps on dims $<16$.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/more-subslices.png}
\end{figure}

\textbf{This nearest-neighbor connectivity is a key difference between TPUs and GPUs}. GPUs are connected with a hierarchy of switches that approximate a point-to-point connection between every GPU, rather than using local connections like a TPU. Typically, GPUs within a node (8 GPUs for H100 or as many as 72 for B200 NVL72) are directly connected, while larger topologies require O(log(N)) hops between each GPU. On the one hand, that means GPUs can send arbitrary data within a small number of hops. On the other hand, TPUs are dramatically cheaper (since NVLink switches are expensive), simpler to wire together, and can scale to much larger topologies because the number of links per device and the bandwidth per device is constant. Read more here.

\textbf{ICI is very fast relative to DCN, but is still slower than HBM bandwidth.} For instance, a TPU v5p has:

\begin{itemize}
\item \texttt{2.5e12} bytes/s (2.5 TB/s) of HBM bandwidth per chip.
\item \texttt{9e10} bytes/s (90 GB/s) of ICI bandwidth per axis, with 3 axes per chip.\footnote{The page above lists 100 GB/s of bandwidth, which is slightly different from what's listed here. TPU ICI links have slightly different bandwidths depending on the operation being performed. You can generally use the numbers in this doc without worry.}
\item \texttt{6.25e9} bytes/s (6.25 GB/s) of DCN (egress) bandwidth per TPU (via 1-2 NICs on each host).\footnote{TPU v6e has 12.5e9 bytes/s and v5e has 3.125e9 bytes/s.}
\end{itemize}

This means that when we split models across multiple chips, we need to be careful to avoid bottlenecking the MXU with slower cross-device communication.

\textbf{Multi-slice training:} A set of ICI-connected TPUs is called a \textbf{slice}. Different slices can be connected between each other using DCN, for instance to link slices on different pods. Since DCN is a much slower connection than ICI, one should try to limit how much our computation has to wait for data from DCN. DCN is host-to-host, so to transfer buffers from TPU to TPU over DCN, we first need to transfer over PCIe to the host, then egress over the network, then ingress over the target host network, then over PCIe into HBM.

\section*{Key Takeaways}
\addcontentsline{toc}{section}{Key Takeaways}

\begin{itemize}
\item TPUs are simple and can in most cases be thought of as a matrix multiply unit connected to memory (super fast), other chips over ICI (rather fast), and the rest of the datacenter over DCN (somewhat fast).

\item Communication is limited by our various network bandwidths in order of speed:
  \begin{itemize}
  \item HBM bandwidth: Between a TensorCore and its associated HBM.
  \item ICI bandwidth: Between a TPU chip and its nearest 4 or 6 neighbors.
  \item PCIe bandwidth: Between a CPU host and its associated tray(s) of chips.
  \item DCN bandwidth: Between multiple CPU hosts, typically hosts not connected by ICI.
  \end{itemize}

\item \textbf{Within a slice, TPUs are only connected to their nearest neighbors via ICI.} This means communication over ICI between distant chips in a slice needs to hop over the intervening chips first.

\item \textbf{Weight matrices need to be padded to at least size 128} (256 on TPU v6) in both dimensions to fill up the MXU (in fact, smaller axes are padded to 128).

\item \textbf{Lower precision matrix multiplication tends to be faster.} TPUs can do int8 or int4 FLOPs roughly 2x/4x faster than bfloat16 FLOPs for generations that support it. VPU operations are still performed in fp32.

\item To avoid bottlenecking the TPU compute unit, we need to \textbf{make sure the amount of communication across each channel is proportional to its speed}.
\end{itemize}

\subsection*{TPU Specs}
\addcontentsline{toc}{subsection}{TPU Specs}

Here are some specific numbers for our chips:

{\scriptsize
\begin{longtable}{lccccc}
\toprule
\textbf{Model} & \textbf{\shortstack{Pod \\ size}} & \textbf{\shortstack{Host \\ size}} & \textbf{\shortstack{HBM capacity \\ /chip}} & \textbf{\shortstack{HBM BW/chip \\ (bytes/s)}} & \textbf{\shortstack{FLOPs/s/chip \\ (bf16)}} \\
\midrule
TPU v3 & 32x32 & 4x2 & 32GB & 9.0e11 & 1.4e14 \\
TPU v4p & 16x16x16 & 2x2x1 & 32GB & 1.2e12 & 2.75e14 \\
TPU v5p & 16x20x28 & 2x2x1 & 96GB & 2.8e12 & 4.59e14 \\
TPU v5e & 16x16 & 4x2 & 16GB & 8.1e11 & 1.97e14 \\
TPU v6e & 16x16 & 4x2 & 32GB & 1.6e12 & 9.20e14 \\
\bottomrule
\end{longtable}
}

Host size refers to the topology of TPUs connected to a single host (e.g. TPU v5e has a single CPU host connected to 8 TPUs in a 4x2 topology). Here are interconnect figures:

{\scriptsize
\begin{longtable}{lcc}
\toprule
\textbf{Model} & \textbf{ICI BW/link (one-way, bytes/s)} & \textbf{ICI BW/link (bidi, bytes/s)} \\
\midrule
TPU v3 & 1e11 & 2e11 \\
TPU v4p & 4.5e10 & 9e10 \\
TPU v5p & 9e10 & 1.8e11 \\
TPU v5e & 4.5e10 & 9e10 \\
TPU v6e & 9e10 & 1.8e11 \\
\bottomrule
\end{longtable}
}

We include both one-way (unidirectional) bandwidth and bidi (bidirectional) bandwidth since unidirectional bandwidth is more true to the hardware but bidirectional bandwidth occurs more often in equations involving a full ring.\footnote{By bidi (bidirectional) bandwidth we mean the total bytes that can be sent along a single link in both directions, or equally, the total number of outgoing bytes from a single TPU along a particular axis, assuming we can use both links efficiently. This is true when we have a functioning ring, AKA when we have a wraparound connection on the particular axis. This occurs on inference chips when we have a full 16 axis, or on training chips (v*p) when we have an axis which is a multiple of 4. We prefer to use the bidirectional bandwidth because it appears frequently in calculations involving bidirectional comms.}

PCIe bandwidth is typically around \texttt{1.6e10} bytes / second per TPU (\texttt{3.2e10} for TPU v6e), while DCN bandwidth is typically around \texttt{6.25e9} bytes / second per TPU (\texttt{12.5e9} for TPU v6e and \texttt{3.125e9} for TPU v5e).

\section*{Worked Problems}
\addcontentsline{toc}{section}{Worked Problems}

These numbers are a little dry, but they let you make basic roofline estimates for model performance. Let's work a few problems to explain why this is useful. You'll see more examples in Part 3.

\textbf{Question 1 [bounding LLM latency]:} Say you want to sample from a 200B parameter model in bf16 that's split across 32 TPU v4p. How long would it take to load all the parameters from HBM into the systolic array? \emph{Hint: use the numbers above.}

\textbf{Question 2 [TPU details]:} Consider a full TPU v5e pod. How many total CPU hosts are there? How many TPU TensorCores? What is the total FLOPs/s for the whole pod? What is the total HBM? Do the same exercise for TPU v5p pod.

\textbf{Question 3 [PCIe operational intensity]:} Imagine we're forced to store a big weight matrix $A$ of type $\text{bfloat16}[D, F]$, and a batch of activations $x$ of type $\text{bfloat16}[B, D]$ in host DRAM and want to do a matrix multiplication on them. This is running on a single host, and we're using a single TPU v6e chip attached to it. You can assume $B \ll D$, and $F = 4D$ (we'll see in future chapters why these are reasonable assumptions). What is the smallest batch size $B$ we need to remain FLOPs bound over PCIe? Assume PCIe bandwidth of 1.5e10 bytes / second.

\textbf{Question 4 [general matmul latency]:} Let's say we want to multiply a weight matrix int8[16384, 4096] by an activation matrix of size int8[B, 4096] where B is some unknown batch size. Let's say we're on 1 TPU v5e to start.

\begin{enumerate}
\item How long will this multiplication take as a function of B? \emph{Hint: it may help to calculate how long it will take to load the arrays from HBM and how long the multiplication will actually take. Which is bottlenecking you?}
\item What if we wanted to run this operation out of VMEM? How long would it take as a function of B?
\end{enumerate}

\textbf{Question 5 [ICI bandwidth]:} Let's say we have a TPU v5e \texttt{4x4} slice. Let's say we want to send an array of type \texttt{bfloat16[8, 128, 8192]} from \texttt{TPU\{0,0\}} to \texttt{TPU\{3, 3\}}. Let's say the per-hop latency for TPU v5e is $1\mu s$.

\begin{enumerate}
\item How soon will the first byte arrive at its destination?
\item How long will the total transfer take?
\end{enumerate}

\textbf{Question 6 [pulling it all together, hard]:} Imagine you have a big matrix \textbf{A}: \texttt{int8[128 * 1024, 128 * 1024]} sharded evenly across a TPU v5e 4x4 slice but offloaded to host DRAM on each chip. Let's say you want to copy the entire array to TPU\{0, 0\} and multiply it by a vector \texttt{bf16[8, 128 * 1024]}. How long will this take? \emph{Hint: use the numbers above.}

\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\subsection*{Appendix A: More on TPU internals}
\addcontentsline{toc}{subsection}{Appendix A: More on TPU internals}

Here we'll dive more deeply into the internal operations of a TPU. Unless otherwise noted, we'll provide specs for a TPU v5p.

\subsubsection*{VPU}

The VPU is the TPU's vector arithmetic core. The VPU consists of a two dimensional SIMD vector machine (the \textbf{VPU}) that performs elementwise arithmetic operations like vadd (vector addition) or vmax (elementwise max) and a set of vector registers called \textbf{VREGs} that hold data for the VPU and MXU.

\textbf{VREGs:} Each TPU v5p core has 64 32-bit VREGs (32 in TPU v4), giving us a total of about \texttt{64 * 8 * 128 * 4 = 256kB} of VREG memory per core (or 2x this for the whole chip since we have two cores). A TPU v5p can load 3 registers from VMEM each cycle, and write 1 register to VMEM each cycle.

\textbf{VPU:} The VPU is a 2D vector arithmetic unit of shape \texttt{(8, 128)} where the 128 dimension is referred to as lane axis and the dimension of 8 is referred to as the sublane axis. Each (lane, sublane) pair on v5 contains 4 standard floating-point ALUs which are independent of each other. The VPU executes most arithmetic instructions in one cycle in each of its ALUs (like vadd or vector add) with a latency of 2 cycles, so e.g. in v5 you can add 4 pairs of f32 values together from VREGs in each cycle. A typical VPU instruction might look like \texttt{\{v2 = vadd.8x128.f32 v0, v1\}} where v0 and v1 are input VREGs and v2 is an output VREG.

All lanes and sublanes execute the same program every cycle in a pure SIMD manner, but each ALU can perform a different operation. So we can e.g. process 1 vadd and 1 vsub in a single cycle, each of which operates on two full VREGs and writes the output to a third.

\textbf{Pop Quiz [Calculating VPU throughput]:} Using the above information, calculate how many vector FLOPs/s a TPU v5p can perform. A TPU v5p has a clock speed of about 1.75GHz.

\textbf{Reductions:} Generally, communication or reduction across the sublane dimension is easier than across the lane dimension. For instance, the VPU supports an intra-lane shuffle operation that can roll along the axis of size 8 in about a cycle. This can be used to perform efficient reductions along the sublane dimension (just shuffle by 4, 2, and 1 and do 3 pairs of elementwise sums).

Cross-lane reductions are much harder and involve a separate hardware unit called the XLU or ``cross lane unit'', which is slow and fairly expensive.

\textbf{Comparison to GPUs:} For those familiar with NVIDIA GPUs, each ALU in the VPU is analogous to a CUDA core, and a single VPU lane is analogous to a ``Warp Scheduler'', i.e. the set of usually 32 CUDA Cores that perform SIMD arithmetic. Reductions within the lane are pretty easy, but if we need to cross lanes, we need to transit at least VMEM/XLU/SMEM which is much slower. See the GPU section for more details.

\subsubsection*{Scalar Core}

The scalar core is the control unit of the TPU. It fetches and dispatches all instructions and executes transfers from HBM into VMEM, and can be programmed to do scalar metadata work. Because the scalar core is single-threaded, one side-effect of this is that each core of the TPU is only capable of creating one DMA request per cycle.

To put this in context, a single scalar core controls a VPU (consisting of 4096 ALUs), 4 MXUs, 2 XLUs, and multiple DMA engines. The highly skewed nature of control per unit compute is a source of hardware efficiency, but also limits the ability to do data dependent vectorization in any interesting way.