\section{Appendix A: How does this change with GB200?}

Blackwell introduces a bunch of major networking changes, including NVLink 5 with twice the overall NVLink bandwidth (900GB/s). B200 still has 8-GPU nodes, just like H100s, but GB200 systems (which combine B200 GPUs with Grace CPUs) introduce much larger NVLink domain (72 GPUs in NVL72 and in theory up to 576). This bigger NVLink domain also effectively increases the node egress bandwidth, which reduces collective costs above the node level.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/gpu/b200-node.png}
\caption{A diagram showing how a GB200 NVL72 unit is constructed, with 18 switches and 72 GPUs.}
\label{fig:b200-node}
\end{figure}

Within a node, this increased bandwidth (from 450GB/s to 900GB/s) doesn't make much of a difference because we also double the total FLOPs/s of each GPU. Our rooflines mostly stay the same, although because NVLink has much better bandwidth, Expert Parallelism becomes easier.

Beyond a node, things change more. Here's a SuperPod diagram.\footnote{See NVIDIA documentation: \texttt{docs.nvidia.com/dgx-superpod/} \texttt{reference-architecture-scalable-infrastructure-gb200/latest/network-fabrics.html}}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{images/gpu/gb200-superpod.png}
\caption{A diagram showing a GB200 DGX SuperPod of 576 GPUs.}
\label{fig:gb200-superpod}
\end{figure}

As you can see, the per-node egress bandwidth increases to $4 \times 18 \times 400 / 8 = 3.6\text{TB/s}$, up from 400GB/s in H100. This improves the effective cross-node rooflines by about 4x since our FLOPs/chip also double. Now we may start to worry about whether we're bottlenecked at the node level rather than the scale-out level.

\textbf{Grace Hopper:} NVIDIA also sells GH200 and GB200 systems which pair some number of GPUs with a Grace CPU. For instance, a GH200 has 1 H200 and 1 Grace CPU, while a GB200 system has 2 B200s and 1 Grace CPU. An advantage of this system is that the CPU is connected to the GPUs using a full bandwidth NVLink connection (called NVLink C2C), so you have very high CPU to GPU bandwidth, useful for offloading parameters to host RAM. In other words, for any given GPU, the bandwidth to reach host memory is identical to reaching another GPU's HBM.
